{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7e4acb-0645-47bd-a5c4-d468af53411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/HiDream-ai/HiDream-I1.git\n",
    "%cd HiDream-I1\n",
    "!apt-get update\n",
    "!apt-get install -y cuda-toolkit-12-4\n",
    "!pip install -r requirements.txt\n",
    "!pip install -U flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0da7796-bb5a-48cb-bf30-2d4650fe0d54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34244d85-2d87-4b48-8c29-342620a0ab45",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd HiDream-I1\n",
    "import torch\n",
    "from hi_diffusers import HiDreamImagePipeline\n",
    "from hi_diffusers import HiDreamImageTransformer2DModel\n",
    "from hi_diffusers.schedulers.fm_solvers_unipc import FlowUniPCMultistepScheduler\n",
    "from hi_diffusers.schedulers.flash_flow_match import FlashFlowMatchEulerDiscreteScheduler\n",
    "from transformers import LlamaForCausalLM, PreTrainedTokenizerFast\n",
    "# ***** Make sure accelerate is installed *****\n",
    "try:\n",
    "    import accelerate\n",
    "except ImportError:\n",
    "    raise ImportError(\"Please install accelerate: pip install accelerate\")\n",
    "\n",
    "model_type = \"full\"\n",
    "MODEL_PREFIX = \"HiDream-ai\"\n",
    "LLAMA_MODEL_NAME = \"mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated\"\n",
    "\n",
    "# Model configurations (keep as is)\n",
    "MODEL_CONFIGS = {\n",
    "    \"dev\": {\n",
    "        \"path\": f\"{MODEL_PREFIX}/HiDream-I1-Dev\",\n",
    "        \"guidance_scale\": 0.0,\n",
    "        \"num_inference_steps\": 28,\n",
    "        \"shift\": 6.0,\n",
    "        \"scheduler\": FlashFlowMatchEulerDiscreteScheduler\n",
    "    },\n",
    "    \"full\": {\n",
    "        \"path\": f\"{MODEL_PREFIX}/HiDream-I1-Full\",\n",
    "        \"guidance_scale\": 5.0,\n",
    "        \"num_inference_steps\": 50,\n",
    "        \"shift\": 3.0,\n",
    "        \"scheduler\": FlowUniPCMultistepScheduler\n",
    "    },\n",
    "    \"fast\": {\n",
    "        \"path\": f\"{MODEL_PREFIX}/HiDream-I1-Fast\",\n",
    "        \"guidance_scale\": 0.0,\n",
    "        \"num_inference_steps\": 16,\n",
    "        \"shift\": 3.0,\n",
    "        \"scheduler\": FlashFlowMatchEulerDiscreteScheduler\n",
    "    }\n",
    "}\n",
    "\n",
    "# Resolution options (keep as is)\n",
    "RESOLUTION_OPTIONS = [\n",
    "    # ... (options remain the same)\n",
    "]\n",
    "\n",
    "# Load models (MODIFIED FUNCTION)\n",
    "def load_models(model_type):\n",
    "    config = MODEL_CONFIGS[model_type]\n",
    "    pretrained_model_name_or_path = config[\"path\"]\n",
    "    scheduler = MODEL_CONFIGS[model_type][\"scheduler\"](num_train_timesteps=1000, shift=config[\"shift\"], use_dynamic_shifting=False)\n",
    "    \n",
    "    max_memory_map = {\n",
    "        0: \"45GiB\",  # Max memory for GPU 0\n",
    "        1: \"45GiB\",  # Max memory for GPU 1\n",
    "        \"cpu\": \"0GiB\" # Disallow model parameter allocation on CPU\n",
    "    }    \n",
    "    \n",
    "    tokenizer_4 = PreTrainedTokenizerFast.from_pretrained(\n",
    "        LLAMA_MODEL_NAME,\n",
    "        use_fast=False,\n",
    "        cache_dir = \"models\")\n",
    "\n",
    "    print(\"Loading Text Encoder (distributing across GPUs)...\")\n",
    "    # Load text encoder with automatic device mapping\n",
    "    text_encoder_4 = LlamaForCausalLM.from_pretrained(\n",
    "        LLAMA_MODEL_NAME,\n",
    "        output_hidden_states=True,\n",
    "        output_attentions=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        cache_dir = \"models\",\n",
    "        # ***** Use accelerate's automatic device map *****\n",
    "        device_map=\"auto\",\n",
    "        max_memory=max_memory_map \n",
    "        # ***** REMOVE .to(\"cuda\") *****\n",
    "    )\n",
    "    print(\"Text Encoder loaded.\")\n",
    "\n",
    "    print(\"Loading Transformer (distributing across GPUs)...\")\n",
    "    # Load transformer with automatic device mapping\n",
    "    transformer = HiDreamImageTransformer2DModel.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        subfolder=\"transformer\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        cache_dir = \"models\",\n",
    "        # ***** Use accelerate's automatic device map *****\n",
    "        device_map=\"auto\",\n",
    "        max_memory=max_memory_map \n",
    "        # ***** REMOVE .to(\"cuda\") *****\n",
    "    )\n",
    "    print(\"Transformer loaded.\")\n",
    "\n",
    "\n",
    "    print(\"Loading Pipeline...\")\n",
    "    # Load the pipeline, passing the already distributed components\n",
    "    # Do NOT use .to(\"cuda\") on the pipeline itself, as its components are already mapped\n",
    "    pipe = HiDreamImagePipeline.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        scheduler=scheduler,\n",
    "        tokenizer_4=tokenizer_4,\n",
    "        text_encoder_4=text_encoder_4, # Pass the distributed text encoder\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        cache_dir = \"models\"\n",
    "        # ***** REMOVE .to(\"cuda\", torch.bfloat16) *****\n",
    "    )\n",
    "    # Assign the distributed transformer\n",
    "    pipe.transformer = transformer\n",
    "    print(\"Pipeline loaded.\")\n",
    "\n",
    "    # Note: The pipeline itself doesn't need .to(device) because its main\n",
    "    # heavy components (text_encoder, transformer) are already distributed\n",
    "    # via device_map=\"auto\". Accelerate hooks handle moving data correctly.\n",
    "\n",
    "    # --- FIX START ---\n",
    "    # Explicitly move the VAE component to the GPU\n",
    "    # Use the device of one of the already distributed components as the target\n",
    "    target_device = pipe.transformer.device # Or text_encoder_4.device, or just \"cuda:0\"\n",
    "    print(f\"Moving VAE to device: {target_device}...\")\n",
    "    pipe.vae.to(target_device)\n",
    "    print(\"VAE moved.\")\n",
    "    # --- FIX END ---\n",
    "    \n",
    "    return pipe, config\n",
    "\n",
    "# Parse resolution function (keep as is)\n",
    "def parse_resolution(resolution_str):\n",
    "    # ... (function remains the same)\n",
    "    pass\n",
    "\n",
    "# Generate image function (keep as is - Generator targeting \"cuda\" is fine,\n",
    "# it usually defaults to cuda:0 for generation init, accelerate handles the rest)\n",
    "def generate_image(pipe, model_type, prompt, resolution, seed):\n",
    "    config = MODEL_CONFIGS[model_type]\n",
    "    guidance_scale = config[\"guidance_scale\"]\n",
    "    num_inference_steps = config[\"num_inference_steps\"]\n",
    "    height, width = resolution #parse_resolution(resolution)\n",
    "\n",
    "    if seed == -1:\n",
    "        seed = torch.randint(0, 1000000, (1,)).item()\n",
    "\n",
    "    # Generator on default cuda device is fine\n",
    "    generator = torch.Generator(\"cuda\").manual_seed(seed)\n",
    "\n",
    "    print(\"REached this point\")\n",
    "\n",
    "    print(f\"Generating image with seed: {seed}\")\n",
    "    images = pipe(\n",
    "        prompt,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        guidance_scale=guidance_scale,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        num_images_per_prompt=1,\n",
    "        generator=generator\n",
    "    ).images\n",
    "\n",
    "    return images[0], seed\n",
    "\n",
    "# --- Main Execution ---\n",
    "print(\"Loading default model (full)...\")\n",
    "pipe, _ = load_models(model_type)\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aef3096-b634-47ef-acbc-6a5c18220611",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A cat holding a sign that says \\\"Hi-Dreams.ai\\\".\"\n",
    "resolution = 1024, 1024\n",
    "seed = -1 # Use -1 for random seed\n",
    "\n",
    "image, used_seed = generate_image(pipe, model_type, prompt, resolution, seed)\n",
    "\n",
    "print(f\"Image generated with seed: {used_seed}\")\n",
    "output_filename = f\"output_seed_{used_seed}.png\"\n",
    "image.save(output_filename)\n",
    "print(f\"Image saved to {output_filename}\")\n",
    "\n",
    "image.show(title=f\"HiDream Result (Seed: {used_seed})\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
